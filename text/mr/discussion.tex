\section{Discussion}
% \begin{itemize}
%     \item In this paper, we present marginal ratio estimator (MR) for off-policy evaluation which only considers the shift in the marginal distribution of the rewards resulting from the policy shift.
%     \item We show, theoretically and empirically, that the proposed method achieves better variance and MSE than the current SOTA methods, and is more data efficient overall.
%     \item We also show that our proposal can be used in the setting of causal inference, and provides more accurate results than the most commonly used methods.
% \end{itemize}

In this paper, we proposed an OPE method for contextual bandits called marginal ratio (MR) estimator, which considers only the shift in the marginal distribution of the outcomes resulting from the policy shift. Our theoretical and empirical analysis showed that MR achieves better variance and MSE compared to the current state-of-the-art methods and is more data efficient overall. Additionally, we demonstrated that MR applied to ATE estimation provides more accurate results than most commonly used methods. Next, we discuss limitations of our methodology and possible avenues for future work.

\myparagraph{Limitations}
The MR estimator requires the additional step of estimating $\hat{w}(y)$ which may introduce an additional source of bias in the value estimation. However, $\hat{w}(y)$ can be estimated by solving a simple 1d regression problem, and as we show empirically in Appendix \ref{app:experiments}, MR achieves the smallest bias among all baselines considered in most cases. Most notably, our ablation study in Appendix \ref{subsec:mips-empirical} shows that even when the training data is reasonably small, MR outperforms the baselines considered. 
% When the behaviour policy $\beh$ is unknown, MR estimator requires the splitting of training data to first estimate $\beh$ and subsequently to use this to estimate marginal weights $w(y)$. This data splitting can be costly in low-data settings, where we do not have access to large training datasets. However, as we empirically show in Appendix \ref{subsec:additional-experiments}, even when the training data is small, MR outperforms the baselines.  


\myparagraph{Future work}
The MR estimator can also be applied to policy optimisation problems, where the data collected using an `old' policy is used to learn a new policy. This approach has been used in Proximal Policy Optimisation (PPO) \citep{schulman2017proximal} for example, which has gained immense popularity and has been applied to reinforcement learning with human feedback (RLHF) \citep{lambert2022illustrating}. We believe that the MR estimator applied to these methodologies could lead to improvements in the stability and convergence of these optimisation schemes, given its favourable variance properties.

